  공간 기반 아키텍처 스타일은 높은 확장성, 탄력성, 동시성과 관련된 문제를 해결하기 위해 설계된 아키텍처 스타일입니다.

# 1. 토폴로지

![image-20240219214323747](images/2장 우분투 데스크톱/image-20240219214323747.png)

  공간기반 아키텍처는 애플리케이션 코드가 구현된 처리 장치, 처리 장치를 관리/조정하는 가상 미들웽, 업데이트된 데이터를 데이터베이스에 비동기 전송하는 데이터 펌프, 데이터 펌프에서 데이터를 받아 업데이트를 수행하는 데이터 라이터, 처리 장치가 시자고디자마자 데이터베이스의 데이터를 읽어 전달하는 데이터 리더 컴포넌트로 구성됩니다.

## 1.1 처리 장치

  처리 장치는 애플리케이션 로직을 갖고 있습니다. 

![image-20240219215055141](images/2장 우분투 데스크톱/image-20240219215055141.png)

## 1.2 가상 미들웨어

  가상 미들웨어는 아키텍처 내부에서 데이터 동기화 및 요청 처리의 다양한 부분을 제어하는 인프라를 담당합니다. 

### 1.2.1 메시징 그리드

![image-20240219215632369](images/2장 우분투 데스크톱/image-20240219215632369.png)

  메시징 그리드는 입력 요청과 세션 상태를 관리합니다. 이 컴포넌트는 보통 부하 분산이 가능한 웹 서버로 구현합니다.

### 1.2.2 데이터 그리드

  데이터 그리드는 대부분 복제 캐시로서 처리 장치에 구현되어 있지만, 외부 컨트롤러가 필요한 복제 캐시 구현체나 분산 캐시를 사용할 경우 데이터 그리드는 가상 미들웨어 내부의 데이터 그리드 컴포넌드와 처리 장치 모두에 위치합니다. 

![image-20240219220022150](images/2장 우분투 데스크톱/image-20240219220022150.png)

### 1.2.3 처리 그리드

  처리 그리드는 필수 컴포넌트는 아니지만, 다수의 처리 장치가 단일 비즈니스 요청을 처리할 경우 요청 처리를 오케스트레이트하는 일을 합니다.

### 1.2.4 배포 관리자

  배포 관리자는 부하 조건에 따라 처리 장치 인스턴스는 동적으로 시작/종료하는 컴포넌트입니다. 탄력성 요구사항을 구현하는 데 꼭 필요한 컴포넌트입니다. 

## 1.3 데이터 펌프

  데이터 펌프는 데이터를 다른 프로세서에 보내 데이터베이스를 업데이트하는 장치입니다. 데이터 펌프는 항상 비동기로 동작하면서 메모리 캐시와 데이터베이스의 최종 일관성을 실현합니다.

![image-20240219221955845](images/2장 우분투 데스크톱/image-20240219221955845.png)

  위 그림처럼 데이터 펌프는 대개 메시징 기법으로 구현합니다. 그리고 데이터 펌프는 데이터와 연관된 액션(추가, 삭제, 수정) 을 포함함니다. 업데이트 데이터는 보통 데이터 펌프 안에서 새 데이터 값만 보관합니다.

## 1.4 데이터 라이터

  데이터 펌프에서 메시지를 받아 그에 맞게 데이터베이스를 업데이트하는 컴포넌트입니다. 도메인 기반의 데이터 라이터는 데이터 펌프 수와 무관하게 특정 도메인의 전체 업데이트를 처리하는 데 필요한 모든 로직을 갖고 있습니다.

![image-20240219222804489](images/2장 우분투 데스크톱/image-20240219222804489.png)

  또는 처리 장치 클래스마다 자체 전용 데이터 라이터를 둘 수도 있습니다. 이 모델은 데이터 라이터가 너무 많은 단점이 있지만 처리 장치, 데이터 펌프, 데이터 라이터가 나란히 정렬되어 확장성, 민첩성은 더 좋습니다.

![image-20240219222915892](images/2장 우분투 데스크톱/image-20240219222915892.png)

## 1.5 데이터 리더

  데이터 리더는 데이터베이스에서 데이터를 읽어 처리 장치로 실어 나르는 컴포넌트입니다. 공간 기반 아키텍처에서 데이터 리더는 세 가지 경우에만 작동합니다.

1. 동일한 이름의 캐시를 가진 모든 처리 장치 인스턴스가 실패하는 경우
2. 동일한 이름의 캐시 안에서 모든 처리 장치를 재배포하는 경우
3. 복제 캐시에 들어있지 않은 아카이브 데이터를 조회하는 경우

# 2. 데이터 충돌

  데이터 충돌은 한 캐시 인스턴스(캐시 A) 에서 데이터가 업데이트되어 다른 캐시 인스턴스(캐시 B) 에 복제하는 도중에 동일한 데이터가 해당 캐시(캐시 B) 에서 업데이트되는 현상을 말합니다. 결국 캐시 B 의 로컬 업데이트는 캐시 A 에서 복제된 옛 데이터 때문에 덮어씌워지고, 반대로 캐시 A 는 동일한 데이터가 캐시 B 에서 발생한 업데이트떄문에 덮어씌워집니다.

![image-20240219224044183](images/2장 우분투 데스크톱/image-20240219224044183.png)

  충돌률은 위 공식으로 구할 수 있습니다. N 은 동일한 이름의 캐시를 사용하는 인스턴스 수, UR 은 밀리초 당 업데이트율, S 는 캐시 크기(로우 개수), RL 은 캐시 제품의 복제대기 시간입니다. 여기서 모든 값은 비례관계이고, 캐시 크기만 반비례 관계입니다.

# 3. 클라우드 대 온프레미스 구현

![image-20240219224247673](images/2장 우분투 데스크톱/image-20240219224247673.png)

  공간 기반 아키텍처는 클라우드와 온프레미스에 동시에 배포할 수 있습니다. 예를 들어 물리 DB 와 데이터를 온프레미스에 그대로 둔 상태로 클라우드 기반 managed 환경에서 처리 장치와 가상 미들웨어를 통해 애플리케이션 배포가 가능합니다.

# 4. 복제 캐시 대 분산 캐시

  복제 캐시는 각 처리 장치에 이름이 동일한 캐시를 동기화하고 자체 인메모리 데이터 그리드에 보관합니다. 한 처리 장치에서 캐시가 업데이트되면 다른 처리 장치도 새로운 데이터로 자동 업데이트되는 구조입니다.

![image-20240219225347099](images/2장 우분투 데스크톱/image-20240219225347099.png)

  복제 캐시는 단일 장애점이 없고 속도가 매우 빠릅니다. 하지만 데이터량이 엄청나게 많거나 캐시 데이터가 너무 빈번하게 업데이트되는 등 복제 캐시를 사용할 수 없는 경우도 있습니다. 이럴 때 분산 캐시를 사용할 수 있습니다. 

![image-20240219225554189](images/2장 우분투 데스크톱/image-20240219225554189.png)

  분산 캐시를 구현하려면 중앙 캐시를 갖고 있는 전용 외부 서버 또는 서비스가 필요합니다. 분산 캐시는 높은 수준의 데이터 일관성을 보장하지만, 캐시 데이터를 원격에서 가져와야 하므로 복제 캐시보다 성능이 낮고 시스템 전체 레이턴시가 증가합니다. 또한 캐시 서버가 다운되었을 때의 내고장성도 문제입니다.

  따라서 복제 캐시와 분산 캐시의 선택은 성능/내고장성과 데이터 일관성 사이의 선택입니다. 따라서 제품 재고와 같이 일관성이 중요한 데이터는 분산 캐시를 사용하고, 자주 변경되지 않는 데이터(이름/값, 제품 코드, 설명 등) 는 복제 캐시를 사용하는 것이 좋습니다.

![image-20240219225902695](images/2장 우분투 데스크톱/image-20240219225902695.png)

# 5. 구현 예시

  공간 기반 아키텍처는 유저 수나 요청량이 갑자기 폭증하는 애플리케이션이나 10,000명이 넘는 동시 유저를 처리해야 하는 종류의 애플리케이션에 적합합니다.